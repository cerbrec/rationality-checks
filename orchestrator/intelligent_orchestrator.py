"""
Intelligent Orchestrator
========================

Main orchestrator that integrates the 7-step workflow with rationality verification.
Each step uses verification skills to ensure accuracy during generation, not after.

Architecture:
- Step 1: Goal Template + Domain Detection
- Step 2: Resolution Strategy + Verification Strategy
- Step 3: Information Collection + Fact Checking
- Step 4: Data Processing + World State Verification
- Step 5: Rational Connection + Consistency Checks
- Step 6: AI Prediction + Adversarial Review
- Step 7: Final Artifact + Completeness + Synthesis

Domain Support:
- NIL: College player valuation
- Energy: Manufacturing efficiency (future)
- Medical: Healthcare information (future)
- Generic: General-purpose reports
"""

import json
import os
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import anthropic

# Add parent directory to path for web_search import
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import verification skills
from .verification_skills import (
    WorldStateVerificationSkill,
    FactCheckingSkill,
    EmpiricalTestingSkill,
    AdversarialReviewSkill,
    CompletenessCheckSkill,
    SynthesisSkill
)

# Import web search
try:
    from web_search import WebSearchTool
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    WEB_SEARCH_AVAILABLE = False
    WebSearchTool = None


class Domain(Enum):
    """Supported domains for intelligent report generation"""
    NIL = "nil"  # College player NIL valuation
    ENERGY = "energy"  # Manufacturing energy efficiency
    MEDICAL = "medical"  # Medical information
    GENERIC = "generic"  # General-purpose


@dataclass
class VerificationMetadata:
    """Metadata about verification performed on a claim"""
    claim: str
    step: str  # Which step generated this claim
    verification_methods: List[str]  # Which skills were applied
    confidence: float
    passed: bool
    issues: List[str] = field(default_factory=list)
    evidence: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class IntelligentReport:
    """Final output with verification metadata"""
    domain: str
    goal: Dict[str, Any]
    strategy: Dict[str, Any]
    collected_data: Dict[str, Any]
    processed_data: Dict[str, Any]
    connections: Dict[str, Any]
    prediction: Dict[str, Any]
    final_recommendation: str
    implementation_plan: List[str]
    verification_summary: Dict[str, Any]
    verified_claims: List[VerificationMetadata]
    overall_confidence: float


class IntelligentOrchestrator:
    """
    Main orchestrator that coordinates the 7-step workflow with integrated verification.

    Each agent is equipped with appropriate verification skills that are used
    during generation to ensure accuracy in real-time.
    """

    def __init__(
        self,
        domain: Union[str, Domain] = Domain.GENERIC,
        model: str = "claude-sonnet-4-5-20250929",
        api_key: Optional[str] = None,
        enable_verification: bool = True
    ):
        """
        Initialize the intelligent orchestrator.

        Args:
            domain: Target domain for report generation
            model: Claude model to use
            api_key: Anthropic API key (or use ANTHROPIC_API_KEY env var)
            enable_verification: Enable integrated verification (default: True)
        """
        self.domain = Domain(domain) if isinstance(domain, str) else domain
        self.model = model
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.enable_verification = enable_verification

        if not self.api_key:
            raise ValueError("Anthropic API key not found. Set ANTHROPIC_API_KEY or pass api_key parameter.")

        self.client = anthropic.Anthropic(api_key=self.api_key)

        # Initialize web search if available
        if WEB_SEARCH_AVAILABLE:
            self.web_search = WebSearchTool()
            print(f"  âœ“ Web search enabled (Serper API)")
        else:
            self.web_search = None
            print(f"  âš ï¸  Web search not available (set SERPER_API_KEY to enable)")

        # Create a simple LLM provider wrapper for verification skills
        # Note: verification skills will mostly operate on pre-structured data
        # so we don't need full LLM for basic mathematical checks

        # Initialize verification skills
        self.world_state_skill = WorldStateVerificationSkill(llm_provider=None)  # No LLM needed for math
        self.fact_check_skill = FactCheckingSkill(llm_provider=None)
        self.empirical_test_skill = EmpiricalTestingSkill(llm_provider=None)
        self.adversarial_skill = AdversarialReviewSkill(llm_provider=None)
        self.completeness_skill = CompletenessCheckSkill()
        self.synthesis_skill = SynthesisSkill()

        # Track verification across all steps
        self.verification_results: List[VerificationMetadata] = []

    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from LLM response, handling markdown fences and extra text.

        Args:
            text: LLM response text

        Returns:
            Parsed JSON dict or None if parsing fails
        """
        text = text.strip()

        # Try direct JSON parse first
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Strip markdown code fences
        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()

        # Try again after stripping
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Look for JSON object - find first { and last }
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        return None

    def _call_llm(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        use_extended_thinking: bool = True
    ) -> str:
        """
        Call Claude LLM with given prompt.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            use_extended_thinking: Enable extended thinking mode for complex reasoning

        Returns:
            str: LLM response text
        """
        messages = [{"role": "user", "content": prompt}]

        kwargs = {
            "model": self.model,
            "max_tokens": 4096,  # Increased for better responses
            "messages": messages
        }

        if system_prompt:
            kwargs["system"] = system_prompt

        # Enable extended thinking for Sonnet models
        if use_extended_thinking and "sonnet" in self.model.lower():
            kwargs["thinking"] = {
                "type": "enabled",
                "budget_tokens": 2048
            }

        try:
            response = self.client.messages.create(**kwargs)

            # Extract text from response, handling both regular and thinking modes
            text_parts = []
            for block in response.content:
                if hasattr(block, 'text'):
                    text_parts.append(block.text)

            return "\n".join(text_parts) if text_parts else ""
        except Exception as e:
            # Fallback without extended thinking if it fails
            if use_extended_thinking:
                return self._call_llm(prompt, system_prompt, use_extended_thinking=False)
            raise

    # ========================================================================
    # STEP 1: GOAL TEMPLATE + DOMAIN DETECTION
    # ========================================================================

    def _step1_goal_template(self, query: str) -> Dict[str, Any]:
        """
        Generate goal template and detect domain.

        Args:
            query: User query

        Returns:
            dict: Goal template with objective, constraints, success_criteria
        """
        print("\n" + "=" * 70)
        print("STEP 1: Goal Template + Domain Detection")
        print("=" * 70)

        prompt = f"""Analyze this query and create a structured goal template.

Query: {query}
Detected Domain: {self.domain.value}

Return JSON with these fields:
{{
    "objective": "Clear statement of what needs to be accomplished",
    "domain": "{self.domain.value}",
    "constraints": ["constraint 1", "constraint 2", ...],
    "success_criteria": ["criterion 1", "criterion 2", ...],
    "required_data_types": ["data type 1", "data type 2", ...],
    "verification_requirements": ["what needs verification", ...],
    "predicted_output_format": "description of output format"
}}

Only return valid JSON."""

        response_text = self._call_llm(prompt)

        # Parse JSON
        goal = self._extract_json(response_text)
        if goal:
            print(f"âœ“ Objective: {goal.get('objective', 'N/A')}")
            print(f"âœ“ Domain: {goal.get('domain', self.domain.value)}")
            return goal
        else:
            # Fallback if JSON parsing fails
            print("  âš ï¸  JSON parsing failed, using fallback goal template")
            return {
                "objective": query,
                "domain": self.domain.value,
                "constraints": [],
                "success_criteria": [],
                "required_data_types": [],
                "verification_requirements": [],
                "predicted_output_format": "structured report"
            }

    # ========================================================================
    # STEP 2: RESOLUTION STRATEGY + VERIFICATION STRATEGY
    # ========================================================================

    def _step2_resolution_strategy(self, goal: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate resolution strategy including verification approach.

        Args:
            goal: Goal template from Step 1

        Returns:
            dict: Strategy with approach, phases, verification_plan
        """
        print("\n" + "=" * 70)
        print("STEP 2: Resolution Strategy + Verification Strategy")
        print("=" * 70)

        prompt = f"""Design a resolution strategy for this goal:

Objective: {goal['objective']}
Domain: {goal.get('domain', 'generic')}
Success Criteria: {', '.join(goal.get('success_criteria', []))}
Required Data: {', '.join(goal.get('required_data_types', []))}

Return JSON with:
{{
    "approach": "High-level approach description",
    "phases": ["phase 1", "phase 2", ...],
    "data_requirements": {{"category": "specific data needed", ...}},
    "processing_pipeline": ["step 1", "step 2", ...],
    "verification_checkpoints": ["what to verify at each phase", ...],
    "validation_checkpoints": ["checkpoint 1", "checkpoint 2", ...]
}}

Only return valid JSON."""

        response_text = self._call_llm(prompt)

        strategy = self._extract_json(response_text)
        if strategy:
            print(f"âœ“ Approach: {strategy.get('approach', 'N/A')[:80]}...")
            return strategy
        else:
            print("  âš ï¸  JSON parsing failed, using fallback strategy")
            return {
                "approach": "Standard analytical approach",
                "phases": ["data collection", "analysis", "recommendation"],
                "data_requirements": {},
                "processing_pipeline": [],
                "verification_checkpoints": [],
                "validation_checkpoints": []
            }

    # ========================================================================
    # STEP 3: INFORMATION COLLECTION + FACT CHECKING
    # ========================================================================

    def _step3_information_collection(
        self,
        strategy: Dict[str, Any],
        context: str
    ) -> Dict[str, Any]:
        """
        Collect information and fact-check using web search.

        Args:
            strategy: Strategy from Step 2
            context: Additional context provided by user

        Returns:
            dict: Collected and verified information
        """
        print("\n" + "=" * 70)
        print("STEP 3: Information Collection + Fact Checking")
        print("=" * 70)

        # First, use LLM to identify what information is needed
        prompt = f"""Identify specific factual information needed to complete this task:

Task Objective: {strategy.get('approach', 'Standard analysis')}
Context: {context}
Data Requirements: {json.dumps(strategy.get('data_requirements', {}))}

Return JSON with:
{{
    "search_queries": ["specific search query 1", "specific search query 2", ...],
    "data_points_needed": ["data point 1", "data point 2", ...],
    "expected_sources": ["source type 1", "source type 2", ...]
}}

Make search queries specific and factual. For NIL domain, include: player stats, team info, NIL valuations.
Only return valid JSON."""

        response_text = self._call_llm(prompt)
        search_plan = self._extract_json(response_text)

        collected_data = {}
        web_search_results = []

        # Perform web searches if available
        if self.web_search and search_plan and "search_queries" in search_plan:
            queries = search_plan["search_queries"][:5]  # Limit to 5 searches
            print(f"\n  ðŸ” Performing {len(queries)} web searches...")

            for i, query in enumerate(queries, 1):
                print(f"    {i}. Searching: {query}")
                result = self.web_search.search(query, num_results=3)
                web_search_results.append(result)

                # Extract data from search results
                if result.get("results"):
                    collected_data[f"search_{i}"] = {
                        "query": query,
                        "sources": [r.get("title") for r in result["results"]],
                        "snippets": [r.get("snippet") for r in result["results"]]
                    }

        # Now use LLM to synthesize the search results
        synthesis_prompt = f"""Analyze these web search results and extract relevant data:

Search Results:
{json.dumps(web_search_results, indent=2)[:3000]}

Context: {context}
Data Points Needed: {json.dumps(search_plan.get('data_points_needed', []) if search_plan else [])}

Return JSON with:
{{
    "collected_data": {{"key": "extracted value from searches", ...}},
    "data_quality_score": 0.0-1.0,
    "verified_facts": ["fact 1 (from source)", "fact 2 (from source)", ...],
    "missing_data": ["what couldn't be found", ...],
    "collection_notes": "summary of data collection",
    "claims_to_verify": ["claim needing additional verification", ...]
}}

Only return valid JSON."""

        response_text = self._call_llm(synthesis_prompt)
        collected = self._extract_json(response_text)

        if not collected:
            print("  âš ï¸  JSON parsing failed, using fallback")
            collected = {
                "collected_data": collected_data,
                "data_quality_score": 0.7 if web_search_results else 0.3,
                "verified_facts": [],
                "missing_data": [],
                "collection_notes": f"Performed {len(web_search_results)} web searches"
            }
        else:
            # Merge in the raw search data
            if "collected_data" not in collected:
                collected["collected_data"] = {}
            collected["collected_data"].update(collected_data)

        # Record verification metadata
        if self.enable_verification and collected.get("verified_facts"):
            for fact in collected["verified_facts"][:5]:
                self._record_verification(
                    "step3_collection",
                    fact,
                    {
                        "method": "web_search",
                        "passed": True,
                        "confidence": 0.85,
                        "issues": []
                    }
                )

        quality = collected.get('data_quality_score', 0.5)
        searches = len(web_search_results)
        print(f"âœ“ Data collected from {searches} searches (quality: {quality:.2f})")

        return collected

    # ========================================================================
    # STEP 4: DATA PROCESSING + WORLD STATE VERIFICATION
    # ========================================================================

    def _step4_data_processing(
        self,
        raw_data: Dict[str, Any],
        processing_steps: List[str]
    ) -> Dict[str, Any]:
        """
        Process data and verify mathematical consistency using WorldStateVerificationSkill.

        Args:
            raw_data: Raw collected data
            processing_steps: Processing pipeline from strategy

        Returns:
            dict: Processed and verified data
        """
        print("\n" + "=" * 70)
        print("STEP 4: Data Processing + World State Verification")
        print("=" * 70)

        collected_data = raw_data.get('collected_data', {})
        verified_facts = raw_data.get('verified_facts', [])

        # Simplified processing - extract and structure the key data points
        processed = {
            "processed_data": {},
            "transformations_applied": ["Extracted social media metrics", "Identified verified facts"],
            "data_statistics": {
                "verified_facts_count": len(verified_facts),
                "data_points_collected": len(collected_data)
            },
            "quality_metrics": {
                "data_quality_score": raw_data.get("data_quality_score", 0.5),
                "verification_coverage": len(verified_facts) / max(len(collected_data), 1)
            }
        }

        # Extract quantitative claims for verification
        quantitative_claims = []

        # Look for numerical data in collected data
        for key, value in collected_data.items():
            if isinstance(value, (int, float)):
                quantitative_claims.append({
                    "id": f"claim_{key}",
                    "text": f"{key}: {value}",
                    "propositions": [{
                        "subject": "data",
                        "predicate": key,
                        "value": value
                    }],
                    "constraints": []
                })
            elif isinstance(value, str) and any(char.isdigit() for char in value):
                # Try to extract numbers from strings like "2M followers"
                import re
                numbers = re.findall(r'[\d.]+[KMB]?', value)
                if numbers:
                    quantitative_claims.append({
                        "id": f"claim_{key}",
                        "text": f"{key}: {value}",
                        "propositions": [{
                            "subject": "data",
                            "predicate": key,
                            "value": value
                        }],
                        "constraints": []
                    })

        # World state verification for quantitative claims
        if self.enable_verification and quantitative_claims:
            print(f"\n  ðŸ” Verifying {len(quantitative_claims)} quantitative data points...")
            result = self.world_state_skill.execute(claims=quantitative_claims)
            for claim in quantitative_claims:
                self._record_verification("step4_processing", claim.get("text", ""), result)
            if not result.get("passed", True):
                print(f"  âš ï¸  Issues found: {len(result.get('issues', []))}")

        processed["quantitative_claims"] = quantitative_claims
        print(f"âœ“ Data processed ({len(quantitative_claims)} quantitative data points)")

        return processed

    # ========================================================================
    # HELPER: Record verification results
    # ========================================================================

    def _record_verification(self, step: str, claim: str, result: Dict[str, Any]):
        """Record verification result for tracking"""
        metadata = VerificationMetadata(
            claim=claim[:200],  # Truncate long claims
            step=step,
            verification_methods=[result.get("method", "unknown")],
            confidence=result.get("confidence", 0.5),
            passed=result.get("passed", False),
            issues=result.get("issues", []),
            evidence=result.get("evidence", [])
        )
        self.verification_results.append(metadata)

    # ========================================================================
    # STEP 5, 6, 7: Remaining steps (simplified for now)
    # ========================================================================

    def _step5_rational_connection(
        self,
        processed_data: Dict[str, Any],
        strategy: Dict[str, Any],
        goal: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Connect data to strategy with consistency checks"""
        print("\n" + "=" * 70)
        print("STEP 5: Rational Connection + Consistency Checks")
        print("=" * 70)

        # Use collected data to map to strategy
        collected_data = processed_data.get("collected_data", {})

        prompt = f"""Map the collected data to the resolution strategy and identify logical connections:

Goal: {goal.get('objective', 'N/A')[:200]}
Strategy Phases: {json.dumps(strategy.get('phases', []))[:500]}
Collected Data: {json.dumps(collected_data, indent=2)[:2000]}

Analyze how the collected data supports or relates to each strategy phase.
Identify gaps where data is missing.

Return JSON with:
{{
    "data_strategy_mapping": {{
        "phase_name": {{
            "available_data": ["data point 1", "data point 2", ...],
            "data_sufficiency": 0.0-1.0,
            "gaps": ["missing data 1", ...]
        }},
        ...
    }},
    "logical_connections": [
        {{
            "data_point": "specific data",
            "supports_phase": "phase name",
            "connection_type": "direct_evidence|supporting|contextual",
            "strength": 0.0-1.0
        }},
        ...
    ],
    "relevance_scores": {{
        "overall_data_coverage": 0.0-1.0,
        "critical_data_available": 0.0-1.0,
        "data_quality": 0.0-1.0
    }},
    "key_insights": ["insight 1", "insight 2", ...]
}}

Only return valid JSON."""

        response_text = self._call_llm(prompt)
        connections = self._extract_json(response_text)

        if not connections:
            print("  âš ï¸  JSON parsing failed, using fallback")
            # Create basic mapping from available data
            connections = {
                "data_strategy_mapping": {
                    "social_media_analysis": {
                        "available_data": list(collected_data.keys())[:5],
                        "data_sufficiency": 0.6,
                        "gaps": ["detailed engagement metrics", "competitor analysis"]
                    }
                },
                "logical_connections": [],
                "relevance_scores": {
                    "overall_data_coverage": 0.5,
                    "critical_data_available": 0.6,
                    "data_quality": 0.7
                },
                "key_insights": []
            }

        coverage = connections.get("relevance_scores", {}).get("overall_data_coverage", 0.5)
        print(f"âœ“ Connections established (data coverage: {coverage:.1%})")
        return connections

    def _step6_ai_prediction(
        self,
        connection_data: Dict[str, Any],
        strategy: Dict[str, Any],
        collected_data: Dict[str, Any],
        goal: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate predictions with adversarial review"""
        print("\n" + "=" * 70)
        print("STEP 6: AI Prediction + Adversarial Review")
        print("=" * 70)

        # Extract verified facts for prediction
        verified_facts = collected_data.get("verified_facts", [])
        data_summary = collected_data.get("collected_data", {})

        prompt = f"""Based on the collected and verified data, predict the outcome:

Goal: {goal.get('objective', 'N/A')[:200]}
Strategy: {strategy.get('approach', 'N/A')[:300]}

Verified Facts:
{json.dumps(verified_facts, indent=2)}

Data Summary:
{json.dumps(data_summary, indent=2)[:1500]}

Data Coverage: {connection_data.get('relevance_scores', {}).get('overall_data_coverage', 0.5)}

For NIL domain: Estimate market value based on social media following, performance, and comparables.

Return JSON with:
{{
    "predicted_outcome": "specific prediction with numbers/ranges",
    "confidence_score": 0.0-1.0,
    "success_probability": 0.0-1.0,
    "value_range": {{"low": number, "high": number, "currency": "USD"}},
    "key_value_drivers": ["driver 1", "driver 2", ...],
    "risk_factors": ["risk 1", "risk 2", ...],
    "assumptions": ["assumption 1", "assumption 2", ...],
    "recommendation": "recommendation text",
    "confidence_reasoning": "why this confidence level"
}}

Only return valid JSON."""

        response_text = self._call_llm(prompt)
        prediction = self._extract_json(response_text)

        if not prediction:
            print("  âš ï¸  JSON parsing failed, using fallback")
            prediction = {
                "predicted_outcome": "Unable to generate detailed prediction due to limited data",
                "confidence_score": 0.5,
                "success_probability": 0.6,
                "value_range": {"low": 0, "high": 0, "currency": "USD"},
                "key_value_drivers": [],
                "risk_factors": ["Insufficient data for accurate prediction"],
                "assumptions": [],
                "recommendation": "Collect more data before making final determination"
            }

        # Adversarial review
        if self.enable_verification and prediction.get("assumptions"):
            print("\n  ðŸ” Adversarial review of assumptions...")
            for assumption in prediction.get("assumptions", [])[:3]:
                result = self.adversarial_skill.execute(
                    claim=assumption,
                    assumptions=[]
                )
                self._record_verification("step6_prediction", assumption, result)

        confidence = prediction.get("confidence_score", 0.5)
        print(f"âœ“ Prediction generated (confidence: {confidence:.1%})")
        return prediction

    def _step7_final_artifact(
        self,
        goal: Dict[str, Any],
        strategy: Dict[str, Any],
        collected_data: Dict[str, Any],
        processed_data: Dict[str, Any],
        connections: Dict[str, Any],
        prediction: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate final artifact with completeness check and synthesis"""
        print("\n" + "=" * 70)
        print("STEP 7: Final Artifact + Synthesis")
        print("=" * 70)

        # Synthesize all findings into final recommendation
        verified_facts = collected_data.get("verified_facts", [])
        data_coverage = connections.get("relevance_scores", {}).get("overall_data_coverage", 0.5)

        prompt = f"""Generate a comprehensive final recommendation based on the complete analysis:

Goal: {goal.get('objective', 'N/A')[:200]}

Verified Facts:
{json.dumps(verified_facts, indent=2)}

Prediction:
{json.dumps(prediction, indent=2)[:1000]}

Data Coverage: {data_coverage:.1%}
Missing Data: {json.dumps(collected_data.get('missing_data', [])[:5])}

Return JSON with:
{{
    "final_recommendation": "comprehensive recommendation with specific actions and numbers",
    "implementation_plan": [
        "Step 1: specific action",
        "Step 2: specific action",
        ...
    ],
    "success_factors": ["factor 1", "factor 2", ...],
    "next_steps": ["next step 1", "next step 2", ...],
    "caveats": ["caveat 1", "caveat 2", ...],
    "confidence_assessment": "overall assessment of recommendation quality"
}}

Be specific and actionable. Reference actual data points collected.
Only return valid JSON."""

        response_text = self._call_llm(prompt)
        artifact = self._extract_json(response_text)

        if not artifact:
            print("  âš ï¸  JSON parsing failed, using fallback")
            artifact = {
                "final_recommendation": f"Prediction: {prediction.get('predicted_outcome', 'N/A')}. Confidence: {prediction.get('confidence_score', 0.5):.1%}. Based on {len(verified_facts)} verified facts.",
                "implementation_plan": [
                    "Review and validate all collected data",
                    "Fill critical data gaps identified in analysis",
                    "Execute strategy with adjusted expectations based on data quality"
                ],
                "success_factors": verified_facts[:3] if verified_facts else [],
                "next_steps": [],
                "caveats": collected_data.get("missing_data", [])[:3]
            }

        # Completeness check
        if self.enable_verification:
            print("\n  ðŸ” Completeness check...")
            result = self.completeness_skill.execute(
                analysis=artifact.get("final_recommendation", ""),
                required_elements=goal.get("success_criteria", [])
            )
            self._record_verification("step7_artifact", "Final recommendation", result)

        print(f"âœ“ Final artifact generated")
        return artifact

    # ========================================================================
    # MAIN EXECUTION
    # ========================================================================

    def generate_report(
        self,
        query: str,
        context: str = ""
    ) -> IntelligentReport:
        """
        Generate intelligent report with integrated verification.

        Args:
            query: What to analyze/report on
            context: Additional context

        Returns:
            IntelligentReport: Complete report with verification metadata
        """
        print("\n" + "=" * 70)
        print(f"INTELLIGENT REPORT GENERATION - Domain: {self.domain.value.upper()}")
        print("=" * 70)

        # Execute 7-step workflow with integrated verification
        goal = self._step1_goal_template(query)
        strategy = self._step2_resolution_strategy(goal)
        collected_data = self._step3_information_collection(strategy, context)
        processed_data = self._step4_data_processing(
            collected_data,
            strategy.get("processing_pipeline", [])
        )

        # Pass collected_data through for processing steps
        processed_data["collected_data"] = collected_data.get("collected_data", {})

        connections = self._step5_rational_connection(processed_data, strategy, goal)
        prediction = self._step6_ai_prediction(connections, strategy, collected_data, goal)
        artifact = self._step7_final_artifact(goal, strategy, collected_data, processed_data, connections, prediction)

        # Calculate overall confidence from verification results
        if self.verification_results:
            overall_confidence = sum(v.confidence for v in self.verification_results) / len(self.verification_results)
        else:
            overall_confidence = 0.7  # Default if no verification performed

        # Build verification summary
        verification_summary = {
            "total_verifications": len(self.verification_results),
            "passed": sum(1 for v in self.verification_results if v.passed),
            "failed": sum(1 for v in self.verification_results if not v.passed),
            "average_confidence": overall_confidence,
            "issues_found": sum(len(v.issues) for v in self.verification_results)
        }

        print("\n" + "=" * 70)
        print("VERIFICATION SUMMARY")
        print("=" * 70)
        print(f"âœ“ Verifications performed: {verification_summary['total_verifications']}")
        print(f"âœ“ Passed: {verification_summary['passed']}")
        print(f"âœ“ Failed: {verification_summary['failed']}")
        print(f"âœ“ Overall confidence: {overall_confidence:.2f}")

        # Create final report
        report = IntelligentReport(
            domain=self.domain.value,
            goal=goal,
            strategy=strategy,
            collected_data=collected_data,
            processed_data=processed_data,
            connections=connections,
            prediction=prediction,
            final_recommendation=artifact.get("final_recommendation", ""),
            implementation_plan=artifact.get("implementation_plan", []),
            verification_summary=verification_summary,
            verified_claims=self.verification_results,
            overall_confidence=overall_confidence
        )

        return report
